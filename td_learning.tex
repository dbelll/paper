\section{Reinforcement Learning}

Reinforcement learning is an area within the machine learning field of artificial intelligence.  The goal of reinforcement learning is to produce agents that can successfully choose optimal actions in a problem domain.  The agent learns by observing its current state, taking an action, and receiving a reward.  The reward can be positive, negative or zero and the agent’s goal is to learn how to choose actions to maximize the total reward received over time.

\subsection{Temporal Difference Learning}
The learning approach used here is based on Temporal Difference learning, a model-free approach where the agent does not attempt to build a model of the domain.  Instead, the agent directly learns the present value of future rewards after taking action $a$ from a state $s$, which is represented by the function $Q(s,a)$.  With a good estimate of the  $Q$-function, the agent can determine the best action from a given state by selecting $a$ which maximizes $Q(s,a)$.  The  $Q$-function defines a policy $\pi_Q(s)$ that maps states onto the optimal action according to \eqref{policy}, and the value of a state $V(s)$ according to \eqref{value_fn}.

\begin{equation}
\pi_Q(s)={\operatorname*{argmax}_a}\,Q(s,a)\label{policy}
\end{equation}

\begin{equation}
V_Q(s)={\operatorname*{max}_a}\,Q(s,a)\label{value_fn}
\end{equation}

The $Q$-function can be stated in terms of $R(s,a)$, the expected reward received when taking action $a$ from state $s$, the discount rate ${\gamma}$, and the expected value of the next state, $s^\prime$.

\begin{align}
Q(s,a) &= R(s,a)+\gamma {\operatorname*{E}_{s^\prime}}[V_Q(s^\prime)]\label{value_fn} \\
&= {\operatorname*{E}_{s^\prime}}[R(s,a)+\gamma {\operatorname*{max}_a}Q(s^\prime, a^\prime)] \label{exp_rq}
\end{align}

Equation \eqref{exp_rq} shows that the  $Q$-function can be expressed as the expected value of the reward plus the maximum value of the $Q$-function from the subsequent state.  Whenever an agent takes an action from a state, receives reward and sees the next state, it gets a sample that can be used to improve its estimate of $Q(s,a)$.  The learning rate parameter, $\alpha$, is used to control the degree to which the estimates of $Q(s,a)$ are updated according to equation \eqref{q_update}, where $r$ is the actual reward received.

\begin{equation}
Q(s,a)\gets (1-\alpha)Q(s,a)+\alpha \left(r + \gamma {\operatorname*{max}_{a^\prime}}\,Q(s^\prime, a^\prime)\right) \label{q_update}
\end{equation}

Equation \eqref{q_update} is the basic learning equation for one type of Temporal Difference learning sometimes referred to as  $Q$-learning.

\subsection{Learning the Value Function}
Sometimes the agent knows how to determine the next state after taking an action from the current state.  When this is the case the agent can learn the value function $V(s)$ instead of learning $Q(s,a)$.  The value function defines a policy based on choosing the action that maximizes $V(s^\prime)$, where $s^\prime$ is the known next state after taking action $a$.

\begin{equation}
\pi_V(s)={\operatorname*{argmax}_a}V(s^\prime) \label{policy_v}
\end{equation}

\begin{equation}
V(s)=R(s,a)+\gamma V(s^\prime) \label{v_def}
\end{equation}

Now the agent can use its interactions with the environment to learn the value function using equation \eqref{v_update}.

\begin{equation}
V(s) \gets (1-\alpha )V(s)+\alpha \left(r+\gamma V(s^\prime)\right) \label{v_update}
\end{equation}

Here $\alpha$ is the learning rate and $\gamma$ the discount rate as before.

\subsection{Eligibility Traces}
When learning the $Q$-function, an agent takes an action $a_t$ at time $t$ from a state $s_t$, receives the reward, and observes the next state.  The new information causes the agent to change its estimation of $Q(s_t, a_t)$ using equation \eqref{q_update}.  In principle, the change in the estimated value of $Q(s_t, a_t)$ could then be retroactively used to update the estimated value for $Q(s_{t-1}, a_{t-1})$, the $Q$-value for the state and action that preceded the current one.  Then the additional change in $Q(s_{t-1}, a_{t-1})$ could be used to update $Q(s_{t-2}, a_{t-2})$, etc., propagating the impact of the new information to states and actions further in the past that lead up to the current state.

The impact of a change can be attenuated as it moves further into the past by multiplying by a factor $0<\lambda < 1$.  This method is called TD$(\lambda)$.  To implement TD$(\lambda)$ we first calculate the amount of change, $\delta_t$, and then use that value to update $Q(s_{t-i}, a_{t-i})$ for $i=0,1,2,\dots$

\begin{equation}
\delta_t=\left(r_t+\gamma Q(s_{t+1}, a_{t+1})\right)-Q(s_t, a_t) \label{delta_t}
\end{equation}

\begin{equation}
Q(s_{t-i}, a_{t-i}) \gets Q(s_{t-i}, a_{t-i})+\alpha \lambda^i \gamma^i \delta_t \label{q_update2}
\end{equation}

A computational shortcut uses an $e$-value for each state/action, $e(s,a)$.  When an agent takes an action from a state, the value of $e(s,a)$ is set to $1.0$.  Each time step after that, the $e$-value is decayed by a factor of $\lambda \gamma$ according to equation \eqref{e_update}.

\begin{equation}
e(s,a) \gets \lambda \gamma e(s,a) \label{e_update}
\end{equation}

Then, after calculating $\delta_t$ using equation \eqref{delta_t}, the update rule for all states becomes

\begin{equation}
Q(s,a) \gets Q(s,a) + \alpha e(s,a) \delta_t \label{q_update3}
\end{equation}

\subsection{Other Considerations}
A problem for reinforcement learning in all but the simplest problem domain is the size of the state and action spaces.  Frequently, the size of the state space makes it impossible for the agent to visit all states during the learning process.  The agent must therefore generalize from the portion of the state space it has experienced to estimate the  - or  -functions over the entire domain.  One approach, when the state space is continuous, is to partition the space into a manageable number of cells.  The agent then estimates the function values for each cell. Another approach is to use neural nets to approximate the function of continuous state variables or of an encoding of a high dimensional state.  Each of these approaches is demonstrated in later chapters.

Reinforcement learning can be done in an off-line mode or on-line mode.  Off-line learning means the agent’s rewards received during the learning process do not matter.  The agent’s quality is determined after learning is complete by testing the agent’s optimal policy function and the average reward it will produce.  In on-line learning the agent’s goal is to optimize the reward received while it is learning, a more difficult task.  In this thesis we deal with off-line learning and determine agent quality by pausing the learning process and separately testing the agent’s optimal policy.  Note that sufficiently fast off-line learning methods can be used in an on-line manner by quickly running off-line learning and then selecting the optimal action in the on-line setting.

Reinforcement learning can be applied to a wide variety of problems.  Problem domains for reinforcement learning are classified in a number of ways.  First, problems are classified based on the state information that is available to the agent.  If the agent always sees complete information about the current state of the world the problem the domain is classified as having observable states.  If some information about the current state of the world is hidden from the agent then the problem is called partially observable.  For example, chess is a game with completely observable states, while bridge or poker are domains with partially observable states.  The four problems presented in this thesis have completely observable states.

Another important characteristic of a problem domain is whether the information about the current state that is available to the agent contains all the information that is relevant to predicting the future.  This is referred to as the Markov property and can be stated in probabilistic terms as the future is independent of the past given the present.  All problems in this thesis have the Markov property.  An example of a problem domain that does not have the Markov property is blackjack with a finite deck of cards.  The state may only contain information about the cards currently in play, but the complete history of cards played from the finite deck is relevant to predicting the probabilities of future cards.  Craps is an example of a game that does have the Markov property, as the probabilities in each new game do not depend on prior results.

In general, the agent in a reinforcement learning problem starts with no information about the world it is in, other than the actions it can choose from.  It simply takes an action from the current state and observes the next state that results from that action.  The agent’s only information about how the world works is gathered through its interactions with the problem domain and observing the transitions between states.  This is true for the first three problems in this thesis.  In the last problem, the agent is learning to play a board game through self-play.  The agent does know how the game works.  It is programmed in advance to be able to determine the next state in the game for each of its possible moves from the current state.  It is also programmed with the knowledge of which moves are legal from the current board position.  This approach is common when reinforcement learning is applied to board games.

This brief introduction to reinforcement learning is intended to give a high level view to readers unfamiliar with the topic.  A great source for learning more about reinforcement learning is a textbook by Sutton and Barto ~\cite{Sutton:1998p3536}.
