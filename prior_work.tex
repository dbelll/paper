\section{Prior Work}

Parallelism techniques have been applied to reinforcement learning in the past.  These applications typically ran on CPUs, but more recently, running on GPUs.  There are also a number of different ways parallelism can be applied.

There are many works that combine multiple agents and reinforcement learning to address a multiagent learning problem.  A Comprehensive Survey of Multiagent Reinforcement Learning (Buşoniu, Babuška, and De Schutter, 2008) describes some of the work in this area.  In contract, this thesis addresses what are traditionally single agent learning problems, and parallelism is used to find the highest quality single agent as fast as possible.  The output of the techniques in this thesis is a single agent even though multiple agents may be used in parallel to find it.

V. Palmer (Palmer, 2007) used GPU programming to speed up matrix calculations in the Least Squares Policy Iteration algorithm (Lagoudakis and Parr, 2003) applied to a multi-agent cooperative Pole Balancing problem.  The GPU was used to speed up calculations on a fixed set of training data for from 1 to 4 agents where each agent had 20 basis functions.  Here, the parallelism on the GPU was used to get a computational speed up of the policy evaluation and policy improvement steps of the Least Squares Policy Iteration algorithm for each agent.  The implementation and interaction of multiple agents was done on the CPU.  The batch processing using the GPU was compared to a Matlab implementation and found to improve calculation speed for training sets of at least 400 points.  The GPU calculation time grew much slower than the CPU calculation time as the training set increased and also as the number of agents increased.

In this thesis we use the GPU to implement parallelism across multiple agents.  In Palmer’s study, the GPU was used within each agent to speed up that agent’s calculations, but did not attempt to deal with multiple cooperating agents with varying experience and the issues of data communication on the GPU.  Furthermore, the degree of parallelism applied in thesis is much greater, with agent group sizes in hundreds or thousands compared to a maximum of 4 agents in Palmer.

Grounds and Kudenko used a parallel approach with multiple CPUs for three reinforcement learning problems (Grounds and Kudenko, 2007).  They parallelized across agents and studied the communication issues between multiple agents.  In their approach, each agent communicated its information to all other agents in a staggered fashion over time.  The agents took turns broadcasting their information to the other agents.  The information communicated was limited to the most important parameters learned by that agent.  The importance of a parameter was determined by how much it changed since the previous communication.  Reducing the quantity of data communicated was necessary due to the relatively slow communication speed between CPUs running on separate machines.  They tested from 1 to 16 agents on Pole Balancing, Mountain Car, and a Grid World problem, and demonstrated increasing quality of learning for a given training time as the number of agents increased.

This thesis differs by running the parallel agents on a single GPU instead of multiple CPUs.  The GPU makes it possible to use thousands of agents while Grounds and Kudenko used a maximum of 16 agents.  Kudenko had to severely limit the quantity of information shared between agents because of the high cost of communication between processes running on different CPUs, which is less of a problem for this thesis.  On the GPU the cost of communication is simply the cost of synchronizing all agents at a sharing point and then calculating and storing values in global memory on the GPU.  Communicating between threads on a single GPU can be less time consuming than communicating between processes on different CPUs.
Kretchmar (Kretchmar, 2002) applied a parallel approach to the N-armed Bandit problem, which we introduce in Chapter 4.  Kretchmar used from 1 to 10 agents in parallel and the quality of learning was measured as a function of time steps per agent. After each time step, all agents shared information.  Individual agents kept track of their own experience data and that of all other agents separately.  This approach allowed the agents to share just the data from their own experience while still having access to the combined information.  The results showed improved learning quality as the number of agents increased as a function of the number of steps in the learning process.  Measurements were made as a function of action steps per agent, so total actions taken increased as the number of agents was increased.  Results were not timed so the cost of communication between agents was not reflected in the quality measurements.

Kretchmar did not consider the time cost involved with parallel agents and information sharing, and by sharing after every time step the agents had complete information.  In this thesis, time is an important component when measuring of the quality of learning. In this work agents spent most of the time operating independently, agents only share information periodically, and the agents do not have complete information. 

Evolutionary techniques are a natural subject when multiple agents are used.  Evolutionary methods and reinforcement learning are compared in (Whiteson, Taylor, and Stone, 2010).  In that work the methods are each applied to two learning problems and the problem characteristics which favor reinforcement learning or evolutionary learning are discussed.  Evolutionary methods and reinforcement learning are considered distinct approaches to solving a learning problem.  In this thesis, we combine some evolutionary techniques with the reinforcement learning process implemented by multiple agents, with beneficial results.

The main differences between this thesis and prior work is the use of the GPU to apply massive parallelism to reinforcement learning problems allowing the use of hundreds or thousands of agents. Large numbers of parallel agents are shown to able to improve the reinforcement learning process through a number of techniques.  Empirical measurements are made to demonstrate the improved learning due to information sharing, differentiation, and evolution of hundreds or thousands of agents.
